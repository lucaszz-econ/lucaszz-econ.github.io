---
layout: page
title: Research
---
<br/>
**Job Market Paper**

* High-Dimensional Conditional Density Estimation and Continuous Difference-in-Differences. [[link]](/notes/JMP.pdf){:target="_blank" rel="noopener"}

   **Abstract**: Conditional density enjoys a series representation, with each term being a known function multiplied by its conditional expectation. This structure is especially beneficial in high-dimensional settings, where these conditional expectations can be flexibly estimated using various machine learning methods. However, choosing the right series terms is challenging. We introduce a data-driven estimator using a cross-validation procedure and demonstrate its optimality through an oracle inequality that bounds the estimation error. Beyond our theory-backed estimation strategy, we underscore the extensive role of conditional density in economics, especially as the generalized propensity score in causal inference with continuous treatment. Furthering this discourse, we extend the widely-used difference-in-differences models to accommodate continuous treatment. Specifically, we establish identification, estimation, and inference results for the causal parameter of interest under the double/debiased machine learning framework. To illustrate the practicality of our methods, we revisit two notable empirical studies: <code>Acemoglu and Finkelstein (2008)</code> on technology adoption in U.S. healthcare industries, and <code>Duflo (2001)</code> on the impact of a large-scale development policy in Indonesia.
   
<br/>
**Working Papers**

* Approximate Sparsity Class and Minimax Estimation. (Under revision. [[link]](/notes/minimax_joe.pdf){:target="_blank" rel="noopener"})

   **Abstract**: Motivated by the orthogonal series density estimation in $L^2([0,1],\mu)$, in this project we consider a new class of functions that we call the approximate sparsity class. This new class is characterized by the rate of decay of the individual Fourier coefficients for a given orthonormal basis. We establish the $L^2([0,1],\mu)$ metric entropy of such class, with which we show the minimax rate of convergence. For the density subset in this class, we propose an adaptive density estimator based on a hard-thresholding procedure that achieves this minimax rate up to a $\log$ term.

<br/>
**Works in Progress**

* Deep Neural Network and Bootstrap (with Mingli Chen and Oscar H. Madrid-Padilla).
* Double/Debiased Nonparametric Counterfactual Distribution Estimation.
