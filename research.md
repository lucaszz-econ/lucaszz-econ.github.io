---
layout: page
title: Research
---
<br/>
**Job Market Paper**

* High-Dimensional Conditional Density Estimation and Continuous Difference-in-Differences Models. [[link]](/notes/JMP.pdf){:target="_blank" rel="noopener"}

   **Abstract**: Conditional density can be shown to enjoy a series representation, in which each series term is a known function multiplied by its conditional expectation. Such structure is particularly useful in the high-dimensional setting as the problem of estimating condi- tional density is then “reduced” to estimating (infinitely) many conditional expectations. To address the challenge of selecting appropriate series terms, we propose a data-driven estimator employing a cross-validation procedure. We demonstrate the optimality of our estimator by establishing an oracle inequality on the estimation error. Conditional density finds wide-ranging applications in economics, notably in causal inference with continuous treatment, where it plays a critical role as the generalized propensity score. To contribute to this literature, we extend the widely-used difference-in-differences models to accommodate continuous treatment, establishing identification, estimation, and inference results under the double/debiased machine learning framework. We illustrate our methods by re-examining two empirical studies: Duflo (2001)’s investigation of a large-scale policy intervention in Indonesia, and Acemoglu and Finkelstein (2008)’s study on technological innovation in the U.S. healthcare industries.
   
<br/>
**Working Papers**

* Approximate Sparsity Class and Minimax Estimation. (Under revision. [[link]](/notes/minimax_joe.pdf){:target="_blank" rel="noopener"})

   **Abstract**: Motivated by the orthogonal series density estimation in $L^2([0,1],\mu)$, in this project we consider a new class of functions that we call the approximate sparsity class. This new class is characterized by the rate of decay of the individual Fourier coefficients for a given orthonormal basis. We establish the $L^2([0,1],\mu)$ metric entropy of such class, with which we show the minimax rate of convergence. For the density subset in this class, we propose an adaptive density estimator based on a hard-thresholding procedure that achieves this minimax rate up to a $\log$ term.

<br/>
**Works in Progress**

* Deep Neural Network and Bootstrap (with Mingli Chen and Oscar H. Madrid-Padilla).
* Double/Debiased Nonparametric Counterfactual Distribution Estimation.
